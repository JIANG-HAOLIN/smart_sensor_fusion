hydra:
  run:
    dir: ./${results_dir}/${task_name}/${datasets.name}/${models.name}${optimizers.name}/exp_${variable_name}/${models.model.model_dim}_${datasets.dataloader.batch_size}/${now:%m-%d-%H:%M:%S}
  sweep:
    dir: ./
    subdir: ${results_dir}/${task_name}/${datasets.name}/${models.name}${optimizers.name}/exp_${variable_name}/${models.model.model_dim}_${datasets.dataloader.batch_size}/${now:%m-%d-%H:%M:%S}
  launcher:
    _target_: hydra._internal.core_plugins.basic_launcher.BasicLauncher
  sweeper:
    _target_: hydra._internal.core_plugins.basic_sweeper.BasicSweeper
    max_batch_size: null
    params:
      models.name: mul2one_newemb
      models.model.model_dim: '256'
      datasets.dataloader.batch_size: '32'
  help:
    app_name: ${hydra.job.name}
    header: '${hydra.help.app_name} is powered by Hydra.

      '
    footer: 'Powered by Hydra (https://hydra.cc)

      Use --hydra-help to view Hydra specific help

      '
    template: '${hydra.help.header}

      == Configuration groups ==

      Compose your configuration from those groups (group=option)


      $APP_CONFIG_GROUPS


      == Config ==

      Override anything in the config (foo.bar=value)


      $CONFIG


      ${hydra.help.footer}

      '
  hydra_help:
    template: 'Hydra (${hydra.runtime.version})

      See https://hydra.cc for more info.


      == Flags ==

      $FLAGS_HELP


      == Configuration groups ==

      Compose your configuration from those groups (For example, append hydra/job_logging=disabled
      to command line)


      $HYDRA_CONFIG_GROUPS


      Use ''--cfg hydra'' to Show the Hydra config.

      '
    hydra_help: ???
  hydra_logging:
    version: 1
    formatters:
      simple:
        format: '[%(asctime)s][HYDRA] %(message)s'
    handlers:
      console:
        class: logging.StreamHandler
        formatter: simple
        stream: ext://sys.stdout
    root:
      level: INFO
      handlers:
      - console
    loggers:
      logging_example:
        level: DEBUG
    disable_existing_loggers: false
  job_logging:
    version: 1
    formatters:
      simple:
        format: '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'
    handlers:
      console:
        class: logging.StreamHandler
        formatter: simple
        stream: ext://sys.stdout
      file:
        class: logging.FileHandler
        formatter: simple
        filename: ${hydra.runtime.output_dir}/${hydra.job.name}.log
    root:
      level: INFO
      handlers:
      - console
      - file
    disable_existing_loggers: false
  env: {}
  mode: MULTIRUN
  searchpath: []
  callbacks: {}
  output_subdir: .hydra
  overrides:
    hydra:
    - hydra.mode=MULTIRUN
    task: []
  job:
    name: train
    chdir: null
    override_dirname: ''
    id: ???
    num: ???
    config_name: config_progress_prediction_vision_audio
    env_set: {}
    env_copy: []
    config:
      override_dirname:
        kv_sep: '='
        item_sep: ','
        exclude_keys: []
  runtime:
    version: 1.3.2
    version_base: '1.2'
    cwd: /home/jin4rng/Documents/code/smart_sensor_fusion
    config_sources:
    - path: hydra.conf
      schema: pkg
      provider: hydra
    - path: /home/jin4rng/Documents/code/smart_sensor_fusion/configs
      schema: file
      provider: main
    - path: ''
      schema: structured
      provider: schema
    output_dir: ???
    choices:
      trainers: progress_predictor
      pl_modules: progress_predictor_vision_audio
      optimizers: adam_steplr
      models: progress_vision_audio/mul2one_vit_audio_vision
      datasets: see_hear_feel_vision_audio
      hydra/env: default
      hydra/callbacks: null
      hydra/job_logging: default
      hydra/hydra_logging: default
      hydra/hydra_help: default
      hydra/help: default
      hydra/sweeper: basic
      hydra/launcher: basic
      hydra/output: default
  verbose: false
datasets:
  name: vision_audio
  dataloader:
    _target_: src.datasets.vision_audio.get_loaders
    batch_size: 128
    data_folder: /home/jin4rng/Documents/code/smart_sensor_fusion/data
    args:
      ablation: vg_ah
      train_csv: train.csv
      val_csv: val.csv
      task: insertion
      num_stack: 5
      frameskip: 5
      no_crop: false
      crop_percent: 0.1
      resized_height_v: 75
      resized_width_v: 100
      resized_height_t: 60
      resized_width_t: 80
      action_dim: 3
      use_flow: false
models:
  name: mul2one_vit_audio_vision
  model:
    _target_: src.models.progress_prediction.VisionAudioFusion
    model_dim: 256
    preprocess_audio_args:
      _target_: src.models.utils.mel_spec.MelSpec
      length: 40000
      sr: 16000
      n_mels: 64
      norm_audio: false
      hop_ratio: 0.05
    tokenization_audio:
      _target_: src.models.utils.tokenization.Vanilla2dTokenization
      channel_size: 1
      patch_size:
      - 64
      - 3
      input_size:
      - 64
      - 51
      model_dim: ${models.model.model_dim}
    pe_audio:
      _target_: src.models.utils.positional_encoding.StandardPositionalEncoding
      d_model: ${models.model.model_dim}
    encoder_audio_args:
      _target_: src.models.transformer_implementations.TransformerEncoder
      token_dim: ${models.model.model_dim}
      num_blocks: 4
      num_heads: 8
      dropout: 0.0
      batch_first: true
      norm_first: true
    preprocess_vision_args:
      _target_: src.models.encoders.identity.get_identity_encoder
    tokenization_vision:
      _target_: src.models.utils.tokenization.Vanilla2dTokenization
      channel_size: 3
      patch_size:
      - 16
      - 16
      input_size:
      - 67
      - 90
      model_dim: ${models.model.model_dim}
    pe_vision:
      _target_: src.models.utils.embeddings.VitPatchEmbedding
      emb_dim: ${models.model.model_dim}
    encoder_vision_args:
      _target_: src.models.transformer_implementations.TransformerEncoder
      token_dim: ${models.model.model_dim}
      num_blocks: 4
      num_heads: 8
      dropout: 0.0
      batch_first: true
      norm_first: true
    last_pos_emb_args:
      _target_: src.models.utils.embeddings.ModalTypeEmbedding
      num_type: 3
      emb_dim: ${models.model.model_dim}
    transformer_classifier_args:
      _target_: src.models.trafo_classifier_vit.TransformerClassifierVitNoPatch
      model_dim: ${models.model.model_dim}
      num_classes: 10
      num_heads: 8
      dropout: 0.0
      input_dropout: 0.0
      num_layers: 4
      add_positional_encoding: false
  inference:
    ckpt_path: ' '
optimizers:
  name: ''
  optimizer:
    _target_: torch.optim.Adam
    lr: 0.0001
  scheduler:
    _target_: torch.optim.lr_scheduler.StepLR
    step_size: 3
    gamma: 1
pl_modules:
  pl_module:
    _target_: src.pl_modules.progress_prediction_vision_audio.TransformerPredictorPl
    num_classes: 10
trainers:
  launch_trainer:
    max_epochs: 25
    monitor: val_acc
task_name: progress_prediction
variable_name: dim_batchsize
seed: 42
results_dir: results
