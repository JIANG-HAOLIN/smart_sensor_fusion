datasets:
  name: dummy
  dataloader:
    _target_: src.datasets.dummy_robot_arm.get_loaders
    batch_size: 32
    data_folder: /fs/scratch/rb_bd_dlp_rng-dl01_cr_ROB_employees/students/jin4rng/data/robot_demo
    drop_last: true
    args:
      ablation: vg_vf
      num_stack: 10
      frameskip: 5
      no_crop: true
      crop_percent: 0.1
      resized_height_v: 240
      resized_width_v: 320
      len_lb: 30
models:
  name: vae
  model:
    _target_: src.models.aloha.build_detrvae
    action_dim: 6
    pose_dim: 7
    style_encoder:
      dilation: false
      position_embedding: sine
      enc_layers: 4
      dec_layers: 7
      dim_feedforward: 1024
      hidden_dim: 256
      dropout: 0.1
      nheads: 8
      pre_norm: false
    action_decoder:
      dilation: false
      position_embedding: sine
      enc_layers: 4
      dec_layers: 7
      dim_feedforward: 1024
      hidden_dim: 256
      dropout: 0.1
      nheads: 8
      num_queries: 30
      pre_norm: false
    obs_encoder:
      _target_: src.models.ssl_nce_framework.SslNceFramework_EarlySum_VATT_addtional
      mod_names:
      - vision
      main_mod: vision
      model_dim: 256
      num_stack: 10
      nce_args:
        norm: batch
        main_mod: ${models.model.obs_encoder.main_mod}
        temp: 1.0
        nce_proj_head:
          _target_: src.models.utils.helpers.ImageBindNceHeader
          model_dim: ${models.model.obs_encoder.model_dim}
          dropout: 0.0
      mask_args:
        mask_type: raw
        masked_mod:
        - vision
        mask_prob:
          latent: 0.5
          vision: 0.5
          audio: 0.08
          tactile: 0.5
        mask_length:
          latent: 1
          vision: 1
          audio: 10
          tactile: 1
      mask_fusion_nce:
        temp: 1.0
        proj_head:
          _target_: src.models.utils.helpers.ImageBindNceHeader
          model_dim: ${models.model.obs_encoder.model_dim}
          dropout: 0.0
      mask_cross_time_trf_nce:
        temp: 1.0
        proj_head:
          _target_: src.models.utils.helpers.ImageBindNceHeader
          model_dim: ${models.model.obs_encoder.model_dim}
          dropout: 0.0
      mask_latent_prediction:
        momentum: false
        loss: mse
        predictor:
          _target_: src.models.utils.header.MLPHead
          in_dim: ${models.model.obs_encoder.model_dim}
          out_dim: ${models.model.obs_encoder.model_dim}
          dropout: 0.0
          norm: layer
      fom_args:
        reorder_prob: 0.2
        predictor:
          _target_: src.models.utils.header.MLPHead
          in_dim: ${models.model.obs_encoder.model_dim}
          out_dim: ${models.model.obs_encoder.num_stack}
          dropout: 0.0
          norm: layer
      audio_args:
        preprocess_audio_args:
          _target_: src.models.utils.mel_spec.MelSpec
          windows_size: 0.05
          length: 80000
          sr: 16000
          n_mels: 64
          norm_audio: false
          hop: 0.05
        tokenization_audio:
          _target_: src.models.utils.tokenization.Vanilla2dTokenization
          channel_size: 1
          input_size:
          - ${models.model.obs_encoder.audio_args.preprocess_audio_args.n_mels}
          - 101
          patch_size:
          - ${models.model.obs_encoder.audio_args.preprocess_audio_args.n_mels}
          - 10
          model_dim: ${models.model.obs_encoder.model_dim}
        pe_audio:
          _target_: src.models.encoders.identity.get_identity_encoder
        encoder_audio_args:
          _target_: src.models.encoders.identity.get_identity_encoder
      vision_args:
        short_window_len: 1
        preprocess_vision_args:
          _target_: src.models.encoders.identity.get_identity_encoder
        tokenization_vision:
          _target_: src.models.vit_implementations.VitVATT3D
          channel_size: 3
          model_dim: ${models.model.obs_encoder.model_dim}
          num_heads: 8
          num_layers: 4
          patch_size:
          - 1
          - 16
          - 16
          input_size:
          - 1
          - 216
          - 576
          num_emb: 500
        pe_vision:
          _target_: src.models.encoders.identity.get_identity_encoder
        encoder_vision_args:
          _target_: src.models.encoders.identity.get_identity_encoder
      tactile_args:
        short_window_len: 1
        preprocess_tactile_args:
          _target_: src.models.encoders.identity.get_identity_encoder
        tokenization_tactile:
          _target_: src.models.vit_implementations.VitVATT3D
          channel_size: 3
          model_dim: ${models.model.obs_encoder.model_dim}
          num_heads: 8
          num_layers: 4
          patch_size:
          - 1
          - 6
          - 6
          input_size:
          - 1
          - 54
          - 72
          num_emb: 109
        pe_tactile:
          _target_: src.models.encoders.identity.get_identity_encoder
        encoder_tactile_args:
          _target_: src.models.encoders.identity.get_identity_encoder
      fusion_args:
        _target_: src.models.utils.fusion.EarlySum
        mod_names: ${models.model.obs_encoder.mod_names}
        dim: ${models.model.obs_encoder.model_dim}
      pos_emb_args:
        _target_: src.models.utils.positional_encoding.StandardPositionalEncoding
        d_model: ${models.model.obs_encoder.model_dim}
      cross_time_trf_args:
        _target_: src.models.transformer_implementations.TransformerEncoderVanilla
        token_dim: ${models.model.obs_encoder.model_dim}
        num_heads: 8
        num_blocks: 4
  inference:
    ckpt_path: ' '
optimizers:
  name: steplr
  optimizer:
    _target_: torch.optim.Adam
    lr: 0.0001
    weight_decay: 0.001
  scheduler:
    _target_: torch.optim.lr_scheduler.StepLR
    step_size: 3
    gamma: 1
pl_modules:
  name: aloha
  pl_module:
    _target_: src.pl_modules.aloha.AlohaPolicy
    num_stack: 5
    t_p: 10
    train_tasks: cross_time_nce+order+recover+imitation
    mask_type: latent_mask
    weight:
      cr_m_nce_loss: 1.0
      masked_fom_loss: 1.0
      mask_fusion_nce_loss: 1.0
      mask_cr_t_nce_loss: 1.0
      recover_loss: 1.0
      fom_loss: 1.0
      kl_divergence: 10
trainers:
  launch_trainer:
    repeat_trial: 2
    max_epochs: 25
    monitor: val_total_loss
    mode: min
    save_top_k: 1
task_name: dummy_aloha
variable_name: pl_modules.name___models.name___models.model.obs_encoder.mask_args.mask_prob.latent
output_name: dummy_aloha
seed: 42
results_dir: /home/jin4rng/Documents/code/smart_sensor_fusion/results
data_folder_path: /fs/scratch/rb_bd_dlp_rng-dl01_cr_ROB_employees/students/jin4rng/data/robot_demo
